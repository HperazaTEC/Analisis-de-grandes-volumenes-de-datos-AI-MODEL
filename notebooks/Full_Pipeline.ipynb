{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# LendingClub Pipeline\nEste notebook integra los agentes principales."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## fetch"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n\"\"\"Download raw LendingClub data from Kaggle with basic validation.\"\"\"\n\nfrom pathlib import Path\nimport os\nimport zipfile\n\nfrom dotenv import load_dotenv\nfrom kaggle import api\nfrom kaggle.rest import ApiException\n\n\ndef main() -> None:\n    load_dotenv()\n\n    dataset = os.environ.get(\"KAGGLE_DATASET\")\n    username = os.environ.get(\"KAGGLE_USERNAME\")\n    key = os.environ.get(\"KAGGLE_KEY\")\n    file_name = os.environ.get(\"KAGGLE_FILE\", \"Loan_status_2007-2020Q3.gzip\")\n\n    raw_dir = Path(\"data/raw\")\n    raw_dir.mkdir(parents=True, exist_ok=True)\n    out_path = raw_dir / file_name\n\n    # Use local copy if present to avoid hitting Kaggle unnecessarily\n    if out_path.exists():\n        print(f\"Using cached dataset at {out_path}\")\n        return\n\n    if not dataset or not username or not key:\n        raise EnvironmentError(\n            \"Kaggle credentials or dataset not configured.\\n\"\n            \"Please set KAGGLE_USERNAME, KAGGLE_KEY and KAGGLE_DATASET in the .env file.\"\n        )\n\n    api.authenticate()\n    try:\n        api.dataset_download_file(dataset, file_name, path=str(raw_dir), force=True)\n    except ApiException as e:\n        raise RuntimeError(\n            \"Failed to download dataset from Kaggle. Check your credentials and dataset permissions.\"\n        ) from e\n    zip_path = raw_dir / f\"{file_name}.zip\"\n    if zip_path.exists():\n        with zipfile.ZipFile(zip_path) as z:\n            z.extractall(path=raw_dir)\n        zip_path.unlink()\n\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## prep"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n\"\"\"Clean raw LendingClub data and generate a processed dataset.\"\"\"\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StringType\nfrom src.utils.spark import get_spark\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nfrom src.utils.metrics import dump_metrics\nimport unicodedata\nimport os\n\n\n\ndef winsorize(df, cols, lower=0.01, upper=0.99):\n    for c in cols:\n        q = df.approxQuantile(c, [lower, upper], 0.05)\n        df = df.withColumn(\n            c,\n            F.when(F.col(c) < q[0], q[0])\n            .when(F.col(c) > q[1], q[1])\n            .otherwise(F.col(c)),\n        )\n    return df\n\n\ndef _normalize(text: str) -> str:\n    if text is None:\n        return text\n    text = unicodedata.normalize(\"NFKD\", text)\n    text = \"\".join(ch for ch in text if not unicodedata.combining(ch))\n    return text.lower()\n\n\n\ndef top_k(df, col, k=100):\n    cats = (\n        df.groupBy(col)\n        .count()\n        .orderBy(F.desc(\"count\"))\n        .limit(k)\n        .collect()\n    )\n    cats = [r[0] for r in cats]\n    return df.withColumn(col, F.when(F.col(col).isin(cats), F.col(col)).otherwise(\"other\"))\n\n\nFAST = os.getenv(\"FAST_MODE\", \"false\").lower() == \"true\"\n\n\ndef main() -> None:\n    load_dotenv()\n    spark = get_spark(\"prep\")\n    src = Path(os.environ.get(\"RAW_DATA\", \"data/raw/Loan_status_2007-2020Q3.gzip\"))\n    proc_dir = Path(\"data/processed\")\n    proc_dir.mkdir(parents=True, exist_ok=True)\n    out_file = proc_dir / (\"M_fast.parquet\" if FAST else \"M_full.parquet\")\n\n    raw_dir = src.parent\n    parts = sorted(raw_dir.glob(\"loan_data_2007_2020Q*.csv\"))\n    if len(parts) >= 3:\n        df = (\n            spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv([str(p) for p in parts])\n        )\n    else:\n        df = (\n            spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"compression\", \"gzip\").csv(str(src))\n        )\n    if FAST:\n        sample_frac = float(os.getenv(\"SAMPLE_FRACTION\", \"0.05\"))\n        df = df.sample(fraction=sample_frac, seed=42)\n    if \"_c0\" in df.columns:\n        df = df.drop(\"_c0\")\n\n    categorical_vars = [\n        \"term\", \"grade\", \"emp_length\", \"home_ownership\",\n        \"verification_status\", \"purpose\", \"loan_status\",\n    ]\n\n    numerical_vars = [\n        \"loan_amnt\", \"int_rate\", \"installment\", \"fico_range_low\", \"fico_range_high\",\n        \"annual_inc\", \"dti\", \"open_acc\", \"total_acc\", \"revol_bal\", \"revol_util\",\n    ]\n\n    percent_cols = [\"int_rate\", \"revol_util\"]\n    for c in percent_cols:\n        df = df.withColumn(c, F.regexp_replace(c, \"%\", \"\").cast(\"double\"))\n    for c in set(numerical_vars) - set(percent_cols):\n        df = df.withColumn(c, F.col(c).cast(\"double\"))\n\n    num_cols = [c for c in df.columns if c in numerical_vars]\n    cat_cols = [c for c in df.columns if c in categorical_vars]\n    medians = {c: df.approxQuantile(c, [0.5], 0.05)[0] for c in num_cols}\n    df = df.fillna(medians)\n    df = df.fillna(\"missing\", subset=cat_cols)\n\n    df = df.withColumn(\"grade_status\", F.concat_ws(\"_\", \"grade\", \"loan_status\"))\n    df = df.withColumn(\n        \"default_flag\",\n        F.when(F.col(\"loan_status\").isin(\"Charged Off\", \"Default\"), 1).otherwise(0),\n    )\n\n    df = winsorize(df, [\"annual_inc\", \"dti\", \"loan_amnt\", \"int_rate\", \"revol_util\"])\n\n    df = df.withColumn(\"loan_to_income\", F.col(\"loan_amnt\") / (F.col(\"annual_inc\") + F.lit(1)))\n    issue_year = F.year(F.to_date(F.concat(F.lit(\"01-\"), F.col(\"issue_d\")), \"dd-MMM-yyyy\"))\n    earliest_year = F.year(F.to_date(F.concat(F.lit(\"01-\"), F.col(\"earliest_cr_line\")), \"dd-MMM-yyyy\"))\n    df = df.withColumn(\"credit_age\", issue_year - earliest_year)\n\n    norm_udf = F.udf(_normalize, StringType())\n    df = df.withColumn(\"emp_title\", norm_udf(F.col(\"emp_title\")))\n\n    for c in cat_cols:\n        df = top_k(df, c, 100)\n\n    df.write.mode(\"overwrite\").parquet(str(out_file))\n\n    dump_metrics(\"prep\", {\"rows\": df.count(), \"fast\": FAST})\n\n\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## register"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"Register the best run in the MLflow Model Registry.\"\"\"\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nfrom dotenv import load_dotenv\n\n\ndef main() -> None:\n    load_dotenv()\n\n    client = MlflowClient()\n    runs = mlflow.search_runs(order_by=[\"metrics.auc DESC\"], max_results=1)\n    if not runs.empty:\n        run_id = runs.loc[0, \"run_id\"]\n        client.create_model_version(name=\"credit-risk\", source=f\"runs:/{run_id}/model\", run_id=run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## split"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"Stratified train/test split by grade and loan_status.\"\"\"\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import DataFrame\nfrom functools import reduce\nfrom src.utils.spark import get_spark\nfrom pathlib import Path\nfrom src.utils.metrics import dump_metrics\nimport os\n\n\ndef stratified_split(df: DataFrame, strat_cols: list, test_frac: float, seed: int):\n    df = df.withColumn(\"stratum\", F.concat_ws(\"_\", *[F.col(c) for c in strat_cols]))\n    strata = [r[0] for r in df.select(\"stratum\").distinct().collect()]\n    train_parts = []\n    test_parts = []\n    for s in strata:\n        sub = df.filter(F.col(\"stratum\") == s)\n        train, test = sub.randomSplit([1-test_frac, test_frac], seed)\n        train_parts.append(train)\n        test_parts.append(test)\n    union = lambda dfs: reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), dfs)\n    train_df = union(train_parts).drop(\"stratum\")\n    test_df = union(test_parts).drop(\"stratum\")\n    return train_df, test_df\n\n\nFAST = os.getenv(\"FAST_MODE\", \"false\").lower() == \"true\"\n\n\ndef main() -> None:\n    spark = get_spark(\"split\")\n    data_path = Path(\"data/processed\") / (\"M_fast.parquet\" if FAST else \"M_full.parquet\")\n    if not data_path.exists():\n        raise FileNotFoundError(f\"{data_path} not found. Run prep.py first.\")\n    df = spark.read.parquet(str(data_path))\n    train, test = stratified_split(df, [\"grade\", \"loan_status\"], test_frac=0.2, seed=42)\n    Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n    n_partitions = 4 if FAST else 8\n    (\n        train.coalesce(n_partitions)\n        .write\n        .option(\"maxRecordsPerFile\", 250000)\n        .mode(\"overwrite\")\n        .parquet(\"data/processed/train.parquet\")\n    )\n    (\n        test.coalesce(n_partitions)\n        .write\n        .option(\"maxRecordsPerFile\", 250000)\n        .mode(\"overwrite\")\n        .parquet(\"data/processed/test.parquet\")\n    )\n\n    dump_metrics(\"split\", {\"train\": train.count(), \"test\": test.count(), \"fast\": FAST})\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## train_sup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"Train supervised models on default_flag.\"\"\"\nimport os\n\n# Disable noisy Spark autologging of datasets\nos.environ[\"MLFLOW_ENABLE_SPARK_DATASET_AUTOLOGGING\"] = \"false\"\n\nfrom pyspark.ml.classification import (\n    RandomForestClassifier,\n    GBTClassifier,\n    MultilayerPerceptronClassifier,\n)\nfrom pyspark.ml.feature import (\n    StringIndexer,\n    OneHotEncoder,\n    VectorAssembler,\n)\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import functions as F\nfrom src.utils.spark import get_spark\nfrom src.utils.balancing import add_weight_column\nfrom src.utils.metrics import METRICS_DIR\nimport mlflow\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nfrom py4j.protocol import Py4JJavaError\nimport logging\nimport sys\nimport re\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    average_precision_score,\n    confusion_matrix,\n    roc_curve,\n    precision_recall_curve,\n)\n\n\n\ndef main() -> int:\n    load_dotenv()\n\n    try:\n        mlflow.spark.autolog()\n    except Exception as e:\n        print(f\"MLflow autologging not available: {e}\")\n\n    FAST = os.getenv(\"FAST_MODE\", \"false\").lower() == \"true\"\n    seed = 42\n\n    spark = get_spark(\"train_sup\")\n    spark.conf.set(\"spark.python.worker.broadcastTimeout\", \"600\")\n    try:\n        train = spark.read.parquet(\"data/processed/train.parquet\")\n        test = spark.read.parquet(\"data/processed/test.parquet\")\n        target = \"default_flag\"\n\n        train = train.cache()\n        test = test.cache()\n    \n        # Auto-detect available driver memory (GB) and compute batch factor\n        mem_str = os.environ.get(\"SPARK_DRIVER_MEMORY\", \"6\")\n        digits = re.findall(r\"\\d+(?:\\.\\d+)?\", mem_str)\n        driver_mem_gb = float(digits[0]) if digits else 6.0\n        max_rows = int(1_000_000 * (driver_mem_gb / 6))\n        total_rows = train.count()\n        if total_rows > max_rows:\n            fraction = max_rows / total_rows\n            train = train.sample(fraction=fraction, seed=42)\n            print(\n                f\"Downsampled train from {total_rows} to {max_rows} rows (driver_mem={driver_mem_gb}G)\"\n            )\n    \n        # Basic preprocessing\n        num_cols = [c for c, t in train.dtypes if t != \"string\" and c != target]\n        cat_cols = [c for c, t in train.dtypes if t == \"string\" and c != target]\n        train = train.fillna(0.0, subset=num_cols)\n        test = test.fillna(0.0, subset=num_cols)\n\n        train = add_weight_column(train, target)\n\n        # Split into train/validation\n        train_df, val_df = train.randomSplit([0.8, 0.2], seed=seed)\n\n        # Preprocessing pipeline: StringIndexer -> OneHotEncoder -> VectorAssembler\n        indexers = [\n            StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n            for c in cat_cols\n        ]\n        encoder = OneHotEncoder(\n            inputCols=[f\"{c}_idx\" for c in cat_cols],\n            outputCols=[f\"{c}_ohe\" for c in cat_cols],\n            dropLast=False,\n        )\n        assembler = VectorAssembler(\n            inputCols=num_cols + [f\"{c}_ohe\" for c in cat_cols],\n            outputCol=\"features\",\n        )\n        prep_pipeline = Pipeline(stages=indexers + [encoder, assembler])\n        prep_model = prep_pipeline.fit(train_df)\n        train_pre = prep_model.transform(train_df)\n\n        n_features = train_pre.select(\"features\").first()[\"features\"].size\n\n        models = {\n            \"RandomForest\": RandomForestClassifier(\n                labelCol=target,\n                featuresCol=\"features\",\n                weightCol=\"weight\",\n                seed=seed,\n            ),\n            \"GBT\": GBTClassifier(\n                labelCol=target,\n                featuresCol=\"features\",\n                weightCol=\"weight\",\n                seed=seed,\n            ),\n        }\n        if not FAST:\n            layers = [n_features, 64, 32, 2]\n            models[\"MLP\"] = MultilayerPerceptronClassifier(\n                labelCol=target,\n                featuresCol=\"features\",\n                layers=layers,\n                seed=seed,\n            )\n\n        results = []\n        Path(\"models/supervised\").mkdir(parents=True, exist_ok=True)\n        for name, algo in models.items():\n            with mlflow.start_run(run_name=name) as run:\n                pipeline = Pipeline(stages=[prep_model, algo])\n                retry_fraction = 1.0\n                while True:\n                    subset = (\n                        train_df\n                        if retry_fraction >= 1.0\n                        else train_df.sample(fraction=retry_fraction, seed=seed)\n                    )\n                    subset = subset.cache()\n                    try:\n                        model = pipeline.fit(subset)\n                        break\n                    except (Py4JJavaError, MemoryError) as e:\n                        if \"java.lang.OutOfMemoryError\" in str(e):\n                            if retry_fraction <= 0.01:\n                                logging.critical(\"Model training failed due to OOM\")\n                                return 1\n                            retry_fraction *= 0.5\n                            print(\n                                f\"OOM detected â†’ retrying fit with fraction={retry_fraction}\"\n                            )\n                        else:\n                            logging.critical(str(e))\n                            return 1\n\n                model.transform(val_df)  # ensure pipeline reused on validation\n                preds = model.transform(test)\n                preds_pd = preds.select(target, \"prediction\", \"probability\").toPandas()\n                y_true = preds_pd[target].astype(int)\n                y_pred = preds_pd[\"prediction\"].astype(int)\n                probs = np.array(preds_pd[\"probability\"].tolist())[:, 1]\n\n                metrics = {\n                    \"auc\": float(roc_auc_score(y_true, probs)),\n                    \"accuracy\": float(accuracy_score(y_true, y_pred)),\n                    \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n                    \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n                    \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n                    \"pr_auc\": float(average_precision_score(y_true, probs)),\n                    \"rows\": subset.count(),\n                    \"fast\": FAST,\n                }\n\n                run_dir = METRICS_DIR / run.info.run_id\n                run_dir.mkdir(parents=True, exist_ok=True)\n\n                with (run_dir / \"metrics.json\").open(\"w\", encoding=\"utf-8\") as f:\n                    json.dump(metrics, f, indent=2)\n\n                cm = confusion_matrix(y_true, y_pred)\n                pd.DataFrame(cm).to_csv(run_dir / \"cmatrix.csv\", index=False)\n                fig, ax = plt.subplots()\n                ax.imshow(cm, cmap=\"Blues\")\n                ax.set_xlabel(\"Predicted\")\n                ax.set_ylabel(\"True\")\n                for i in range(cm.shape[0]):\n                    for j in range(cm.shape[1]):\n                        ax.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\")\n                plt.tight_layout()\n                fig.savefig(run_dir / \"cmatrix.png\")\n                plt.close(fig)\n\n                fpr, tpr, _ = roc_curve(y_true, probs)\n                fig, ax = plt.subplots()\n                ax.plot(fpr, tpr)\n                ax.set_xlabel(\"FPR\")\n                ax.set_ylabel(\"TPR\")\n                plt.tight_layout()\n                fig.savefig(run_dir / \"roc.png\")\n                plt.close(fig)\n\n                prec_c, rec_c, _ = precision_recall_curve(y_true, probs)\n                fig, ax = plt.subplots()\n                ax.plot(rec_c, prec_c)\n                ax.set_xlabel(\"Recall\")\n                ax.set_ylabel(\"Precision\")\n                plt.tight_layout()\n                fig.savefig(run_dir / \"pr.png\")\n                plt.close(fig)\n\n                if hasattr(model.stages[-1], \"featureImportances\"):\n                    fi = model.stages[-1].featureImportances.toArray().tolist()\n                    with (run_dir / \"feature_importance.json\").open(\"w\", encoding=\"utf-8\") as f:\n                        json.dump(fi, f)\n                    mlflow.log_artifact(str(run_dir / \"feature_importance.json\"))\n\n                mlflow.log_metrics({k: metrics[k] for k in [\"auc\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"pr_auc\"]})\n                mlflow.log_artifact(str(run_dir / \"metrics.json\"))\n                mlflow.log_artifact(str(run_dir / \"cmatrix.csv\"))\n                mlflow.log_artifact(str(run_dir / \"cmatrix.png\"))\n                mlflow.log_artifact(str(run_dir / \"roc.png\"))\n                mlflow.log_artifact(str(run_dir / \"pr.png\"))\n\n                mlflow.spark.log_model(\n                    model,\n                    f\"models/supervised/{name}\",\n                    registered_model_name=\"credit-risk\",\n                )\n\n                results.append({\n                    \"model\": name,\n                    **{k: metrics[k] for k in [\"auc\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"pr_auc\"]},\n                })\n\n        summary_df = pd.DataFrame(results)\n        print(summary_df)\n        return 0\n    finally:\n        spark.stop()\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## train_unsup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"Train KMeans clustering model with robust preprocessing.\"\"\"\n\nimport json\nimport os\n\n# Disable dataset autologging noise from mlflow-spark\nos.environ[\"MLFLOW_ENABLE_SPARK_DATASET_AUTOLOGGING\"] = \"false\"\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import (\n    OneHotEncoder,\n    StandardScaler,\n    StringIndexer,\n    VectorAssembler,\n)\nfrom src.utils.spark import get_spark\nfrom src.utils.metrics import METRICS_DIR\nimport mlflow\nfrom dotenv import load_dotenv\n\n\ndef main() -> None:\n    load_dotenv()\n\n    fast = os.getenv(\"FAST_MODE\", \"false\").lower() == \"true\"\n    sample_frac = float(os.getenv(\"SAMPLE_FRACTION\", \"0.05\"))\n\n    mlflow.spark.autolog()\n\n    spark = get_spark(\"train_unsup\")\n    spark.conf.set(\"spark.sql.debug.maxToStringFields\", 50)\n    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n\n    df = spark.read.parquet(\"data/processed/M.parquet\")\n    if fast:\n        df = df.sample(fraction=sample_frac, seed=42)\n\n    num_cols = [c for c, t in df.dtypes if t != \"string\"]\n    cat_cols = [c for c, t in df.dtypes if t == \"string\"]\n\n    indexers = [\n        StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n        for c in cat_cols\n    ]\n    encoder = OneHotEncoder(\n        inputCols=[f\"{c}_idx\" for c in cat_cols],\n        outputCols=[f\"{c}_ohe\" for c in cat_cols],\n        dropLast=False,\n    )\n    assembler = VectorAssembler(\n        inputCols=num_cols + [f\"{c}_ohe\" for c in cat_cols],\n        outputCol=\"raw_feats\",\n    )\n    scaler = StandardScaler(inputCol=\"raw_feats\", outputCol=\"features\")\n    kmeans = KMeans(k=8, seed=42, featuresCol=\"features\", predictionCol=\"cluster\")\n\n    pipeline = Pipeline(stages=indexers + [encoder, assembler, scaler, kmeans])\n\n    with mlflow.start_run(run_name=\"kmeans\"):\n        model = pipeline.fit(df)\n        mlflow.spark.log_model(\n            model,\n            \"models/unsupervised/kmeans\",\n            registered_model_name=\"credit-risk-segmentation\",\n        )\n        inertia = model.stages[-1].summary.trainingCost\n        metrics = {\"k\": 8, \"inertia\": float(inertia), \"rows\": df.count(), \"fast\": fast}\n\n        METRICS_DIR.mkdir(parents=True, exist_ok=True)\n        with (METRICS_DIR / \"kmeans_metrics.json\").open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(metrics, f, indent=2)\n\n        mlflow.log_metrics({\"k\": 8, \"inertia\": metrics[\"inertia\"], \"rows\": metrics[\"rows\"]})\n\n    spark.stop()\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## get_spark"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pyspark.sql import SparkSession\nimport os\nimport re\n\n\ndef get_spark(app_name: str = \"credit-risk\") -> SparkSession:\n    \"\"\"Create or retrieve a Spark session with common config.\"\"\"\n    os.environ.setdefault(\"PYSPARK_PIN_THREAD\", \"false\")\n    os.environ.setdefault(\"SPARK_LOCAL_IP\", \"127.0.0.1\")\n\n    mem_str = os.environ.get(\"SPARK_DRIVER_MEMORY\", \"10g\")\n    digits = re.findall(r\"\\d+(?:\\.\\d+)?\", mem_str)\n    driver_mem_gb = float(digits[0]) if digits else 10.0\n\n\n    builder = (\n        SparkSession.builder\n        .appName(app_name)\n        .config(\"spark.driver.memory\", mem_str)\n        .config(\"spark.executor.memory\", os.environ.get(\"SPARK_EXECUTOR_MEMORY\", mem_str))\n        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n        .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n        .config(\"spark.driver.maxResultSize\", os.environ.get(\"SPARK_DRIVER_MAXRESULTSIZE\", \"3g\"))\n    )\n\n    if driver_mem_gb <= 8:\n        builder = builder.config(\"spark.sql.shuffle.partitions\", \"200\")\n\n    return builder.getOrCreate()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import pandas as pd\n",
        "\n",
        "records = []\n",
        "for name in ['RandomForest', 'GBT']:\n",
        "    df = mlflow.search_runs(filter_string=f\"tags.mlflow.runName = '{name}'\", order_by=['metrics.auc DESC'], max_results=1)\n",
        "    if not df.empty:\n",
        "        records.append(df[[\"run_id\", 'tags.mlflow.runName', 'metrics.auc', 'metrics.accuracy', 'params.maxDepth', 'params.numTrees', 'params.maxIter', 'params.stepSize']])\n",
        "if records:\n",
        "    display(pd.concat(records))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
